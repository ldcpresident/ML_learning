# -*- coding: utf-8 -*-
"""Copy of ML_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19hTi8Yl-SMw5-CLchlHik56kruZNAkC6
"""

import torch
import pandas as pd
import numpy as pd
import matplotlib.pyplot  as plt
print(torch.__version__)
from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css) # this is for word wrap but only works on the output not the code sadly...

TENSOR = torch.tensor([[[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]]])
TENSOR.shape

TENSOR.ndim

TENSOR2 = torch.tensor([[[1, 2, 3],
                         [4, 5, 6]]])
TENSOR2.shape

TENSOR3 = torch.tensor([[[1,2,3]]])
TENSOR3.shape

TENSOR4 = torch.tensor([[[1]]])
TENSOR4.shape

random_tensor = torch.rand(2, 3, 4)
random_tensor

random_image_size_tensor = torch.rand(size=(224, 224, 3)) #height, width, color channel
random_image_size_tensor.shape, random_image_size_tensor.ndim

"""Creating a range of tensors and tensor-like"""

one_to_ten = torch.arange(start=1, end=100, step=5)
one_to_ten

ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros
# creates a range that is like another range

ten_ones = torch.ones_like(input=one_to_ten)
ten_ones

"""Tensor datatypes

** Note: ** Tensor datatypes is one of the 3 big errors you will run into with Pytorch and Deep learning
1. Tensor not right datatype
2. Tensor not right shape
3. Tensor not on the right device
"""

float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None, # what datatype the tesnor is
                               device=None, # what devices is your tensor on eg. CPU, GPU, Cuda
                               requires_grad=False) # does your tensor require gradients with this tensor operations)
float_32_tensor

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

"""Getting information from tensors"""

# `tensor.dtype` gives you the data type for tensor
# `tensor.shape` gives you the shape of the tensor
# `tensor.device` gives you the device of the tensor

some_tensor=torch.rand(3,4)
some_tensor

print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Device tensor is on: {some_tensor.device}")

"""Manipulating Tensors (Tensor Operations)

Tensor Operations ↓
*   Addition
*   Subtraction
*   Multiplication (element-wise)
*   Division
*   Matrix Multiplication


"""

# create a tensor and add 10 to the tensor
tensor = torch.tensor([1, 2, 3])
tensor

tensor_plus100 = tensor + 100
tensor_plus100

# subtract from a tensor
tensor_minus10 = tensor - 10
tensor_minus10

# using pytorch torch.mul
tensor_times10 = torch.mul(tensor, 10)
tensor_times10

"""## matrix multiplication

## two ways to perform multiplication in neural networks and deep learning
## 1. Element-wise multiplication
## 2. Matrix multiplication (dot product)
## both can be found here https://www.mathsisfun.com/algebra/matrix-multiplying.html

Two rules to perform matrix multiplication without an error:
1. The ** inner dimmension ** must match:

* `(3, 2) @ (3, 2)` will not work

* `(2, 3) @ (3, 2)` will work ← the 3 on the insides match so it works

* `(3, 2) @ (2, 3)` will work ← the 2 on the insides match so it works

2. The resulting matrix has the shape of the **outer dimensions**:
  
* `(2, 3) @ (3, 2) → (2, 2)`
  
* `(3,2) @ (2, 3) → (3, 3)`
"""

# Element wise multiplication
print(tensor, "*", tensor)
print(f"Equals: {tensor * tensor}")

# Matrix multiplcation
print(tensor, "matrix multiplied", tensor)
print(f"Equals: (1 x 1) + (2 x 2) + (3 + 3)")
print(torch.matmul(tensor, tensor))

value = 0
for everything in range(len(tensor)):
  value += tensor[everything] * tensor[everything]
  print(value)

"""### One of the most common errors in deep learning: shape error

"""

test_tensor = torch.tensor([10, 6])
test_tensor2 = torch.tensor([6, 10])
# torch.matmul (test_tensor, test_tensor2)
# print(test_tensor @ test_tensor2) @ and torch.matmul are the same operation although matmul is faster

# Shapes for matrix multiplication
tensor_A = torch.tensor([[1, 2,],
                         [3, 4],
                         [5, 6]])
tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])
# torch.mm (tensor_A, tensor_B) # fails because the inner dimmesions are not the same we must transpose them to fix the inner dimmensions so that they can be multiplied
print(torch.mm (tensor_A, tensor_B.T))

tensor_A.shape, tensor_B.shape

"""# Tensor Transpose changing shape of tensors
### having incorrect tensor shape issues with tensor_A and tensor_B

### to fix this we can use **transpose** to switch the axes or dimesions of a tensor
"""

# The matrix multiplication operation works when tensor_B is trnasposed
print(f"Original shapes: \ntensor_A = {tensor_A.shape}, \ntensor_B = {tensor_B.shape}")
print(f"New shapes: \ntensor_A = {tensor_A.shape} (not changed) \ntensor_B.T = {tensor_B.T.shape}")
print(f"Multiplying: \n tensor_A: {tensor_A} \nwith tensor_B: \n{tensor_B.T} \n{tensor_A.shape} @ {tensor_B.T.shape} ← inner shapes are now the same so we can multiply ")
print("Output:\n")
output = torch.matmul(tensor_A, tensor_B.T)
print(output)
print(f"the first column in the output is made up of the first colum of tensor_B matrix multiplied by tensor_A i.e (7 x 1) + (10 x 2) \n the second column would be then everything being multiplied by the second column of tensor_B i.e (8 x 1) + (11 x 2) and so on ")
print(f"\nOutput shape: {output.shape}")

test_tensor = torch.tensor([[1, 1],
                            [1, 2],
                            [1, 3],
                            [1, 4]])
print(test_tensor)
test_tensor.shape
test_tensor2 = torch.tensor([[1, 1, 1, 1],
                             [1, 1, 1, 1],
                             [1, 1, 1, 1]])

# print(test_tensor2)
# torch.matmul(test_tensor, test_tensor2) # this does not work because they are the wrong tensor shapes but technically in math you can multiply matrixes that are not the same shape

"""# Finding the min, max, mean, sum etc (Tensor aggregation)"""

# Create tensor
x = torch.arange(0, 100, 10)
x

# Find the max
torch.max(x), x.max()

# Find the min
torch.min(x), x.min()

# Find the mean
# torch.mean(x), x.mean() # this fails because the data type is not specified here. without specification, it is returing a long

# torch.mean(x.type(torch.float))
x.type(torch.float).mean()

# Find the sum
torch.sum(x)
x.sum()

"""# Find the positional min and max

"""

# Find the position in tensor that has the minimum value with argmin() → returns the index position of the minimum value
print(x.argmin())

# Find the position in the tensor that has the maximum value with argmax() → returns the index position of the maximum value
print(x.argmax())

# Shows what is stored in index 9
x[9]

"""# Changing shape and dimensions of tensors
### Reshaping, Stacking, squeezing and unsqueezing tensors

* Reshaping - reshapes an input tensor to a defined shape
* View - Returns a view of an input tensor of certain shape but keeps the same memory as the original tensor
* Stacking - combines multiple tensors on top of each other (vstack) or side by side (hstack)
* Squeeze - removes all `1` dimensions from a target tensor
* Unsqueeze - add a `1` dimension to a target tensor
* Permute - Return a view of the input with dimensions permute (swapped) in a certain way
"""

# Create a tensor
import torch
x = torch.arange(1., 10.) # arange removes the last item from the range so arange(1,10) means a range of 1 - 10 excluding 10
x, x.shape

# Add an extra dimension
# x_reshaped = x.reshape(1, 7) # this fails because we are trying to put 9 elements in an array that has 7 slots reshape(1, 7) means put the elements in 1 row and 7 columns
x_reshaped = x.reshape(1, 9) # I have a spot for all the elements in the tensor so the reshape works.
x_reshaped

# Change the view
z = x.view(1, 9)
z, z.shape

# Changing z changes x (because a view of a tensor shares the same memory as the original input)
# z[:,0] = 1 # we are changing the element in index 0 here (the first element) to be 5
z, x

# Stack tensors on top of each other
x_stacked = torch.stack([x,x,x,x], dim = 1) # dim = 1 sets the dimension to the 1 setting which looks to be a 1 column setting for each matrix
x_stacked

# torch.squeeze() - removes all single dimensions
x = torch.zeros(2, 1, 2, 1, 2)
x.size()

y = torch.squeeze(x)
y.size()

print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape: {x_reshaped.shape}")

# remove extra dimensions from x_reshaped
x_squeezed =x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}")
print(f"New shape: {x_squeezed.shape}")

# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim (dimension)
print(x_squeezed.shape)
x_unsqueezed =  x_squeezed.unsqueeze(dim = 0)
print(x_unsqueezed.shape)
print(x_unsqueezed)

# torch.permute rearranges the dimensions of a target tensor in a specified order
x_original = torch.randn(size=(224, 224, 3)) # [height, width, color_channel]
x_original.shape
# print(x_original)

# Permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_original.permute(2, 0, 1) # shifts axis 0 → 1, 1 → 2, 2 → 1 changing the index mapping is what is going on here. Index 0 goes to 1 index 1 goes to 2 and index 2 goes to 0
print(f"Previous shape: {x_original.shape}")
print(f"\nNew shape: {x_permuted.shape}")

x_original[0,0,0] = 42069
x_original.shape
x_original2 = x_original.permute(2, 0, 1)
# print(x_original2)
# print(x_original)

# Create a tensor
import torch
x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape

# Let's index on our new tensor
x[0]

# Let's index on the middle bracket (dim = 1)
x[0][0]

# Let's index on the most inner bracket (last dimension)
x[0][0][0]

"""## Indexing Matrices

### The indexing function `x[:]` is what you will use to determine where you want to grab something from a matrix
### for example `x[0, :, :]` will grab everything in the first matrix
* `x[0, 1, :]` will go to the first matrix and grab everything in the second row
* `x[0, :, 1]` will go to the first matrix and grab everything in second column
* `x[0, 1, 2]` will go to the first matrix and then the second row and grab the 3rd item

"""

# Using ":" to get indexes
print(x[:])
print(f"\n{x[:, 0]}")
print(f"\n{x[:, 2, 2]}")
print(f"\n indexing specific columns now: \n {x[:, :, 2]}")

"""# PyTorch tensors and NumPy

Numpy is a popular scientific Python numerical computing library.
And because of this, PyTorch has functionality to interact with it.
* Data in NumPy, want in PyTorch tensor → `torch.from_numpy(name_of_array)`
* PyTorch tensor → NumPy → `torch.Tensor.numpy()`

"""

# NumPy array to tensor
import torch
import numpy as np

array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array) # When converting from a numpy array to a pytorch tensor, the dtype relects the default dtype of numpy which is float.64 so you will need to specify the dtype
array, tensor

# change teh value of array, what will happen to `tensor`
array = array + 1 # this adds one to every value in array
array, tensor # the tensor here is not affected.

# Tensor to NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy() # this changes from a pytorch tensor → numpy array
tensor, numpy_tensor

# Changing the tensor, what happens to the numpy array
tensor = tensor + 1
tensor, numpy_tensor # the numpy array does not change (it is a view) the dtype is the pytorch default, so you will need to specify the dtype here.

"""# Reproducibility (taking the random out of random using torch.manual_seed)

How a neural network learns:

`start with random numbers → tensor operations → update random numbres to try and make them better representations of the data → repeat...`

To reduce the randomness in neural networks and PyTorch comes the concept of a **random seed**.

Essentiall what the random seed does is "flavor" the randomness.
"""

import torch
import numpy as np

# create random tensors
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

# Set the random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED) # we have to call the random_seed again here when we are instantiating a new tensor
random_tensor_D = torch.rand(3, 4)

print(random_tensor_C == random_tensor_D)

"""# Running tensors and PyTorch objects on GPUs (making computations faster)

GPUs= faster computation on numbers, thanks to CUDA + NVIDIA hardware + PyTorch working behind the scenes to make everything good

# 1. Getting a GPU
  1. Easiest - Use Google Colab for free GPU (can upgrade for bigger projects)
  2. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/
  3. Use cloud computing - Google Cloud Platform (GCP), AWS, Azure

for 2, 3 PyTorch + GPU drivers (CUDA) takes a little bit of setting up, to do this, refer to PyTorch setup documentation https://pytorch.org/get-started/locally/

"""

# setup device agnostic code
device = "cude" if torch.cuda.is_available() else "cpu"
device

# Count number of devices
torch.cuda.device_count()

"""# Putting tensors (and models) on the GPU
The reason we want our tensors/models on the GPU is because using a GPU results in faster computations

"""

# create a tensor
tensor = torch.tensor([1, 2 ,3], device = "cpu")

# Tensor not on GPU
print(tensor, tensor.device)

# Move tensor to GPU (if available)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu

"""## Moving Tensors back to CPU"""

# If tensor is on GPU, can't transform it to NumPy
tensor_on_gpu.numpy() # I am a broke boii so I am on CPU but if I was not, this would error and I would have to set this to a GPU

# To fix the GPU tensor with NumPy issue, we can first set it to the CPU
tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu # this works now and we get an array in NumPy (basically just a tensor in PyTorch)

"""# PyTorch Workflow

Let's explore an example PyTorch end to end workflow.
Resources:

* Ground truth notebook → https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb
* Book version of notebook → https://www.learnpytorch.io/01_pytorch_workflow/
* Ask a question → https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb
"""

what_we_are_covering = {1: "Data (prepare and load)",
                        2: "Build model",
                        3: "Fitting the model to data (training)",
                        4: "Making predictions and evaluting a model (inference)",
                        5: "Saving and loading a model",
                        6: "Putting it all together"}

what_we_are_covering

import torch
import numpy as np
import matplotlib.pyplot as plt

# Check PyTorch version
torch.__version__

"""## 1. Data (preparing and loading)

Data can be almost anything in machine learning

* Excel spreadsheet
* Images
* Videos (YouTube has lots of data)
* DNA
* Text

Machine learning is a game of two parts:
1. Get data into numerical representation
2. Build a model that finds patterns in that numerical representation

To showcase this, let's create some kown data using the linear regression formula.
We'll use a linear regression formula to make a straight line with known *parameters*
"""

# Create *known* parameters
weight = 0.7
biase = 0.3

# create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim = 1)
y = weight * X + biase # MX + C the linear regression model formula
X[:10], y[:10]

len(X), len(y)

"""## Splitting data into training and test sets (very important concept)

Let's create a training and test set with our data

one of the most popular training splits is → https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

# Create a train/test split
train_split = int(0.8 *len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

print(f"Data allocated to X_train is: {len(X_train)} \nData allocated to y_train is: {len(y_train)}")
print(f"Data allocated to X_test is: {len(X_test)} \nData allocated to y_test is: {len(y_test)}")

"""## Visualize the data using MatplotLib"""

def plot_predictions(train_data = X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_labels = y_test,
                     predictions = None):
  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10, 7))

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")

  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data")

  if predictions is not None:
    # Plot the predictions in red (predictions were made on the test data)
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")

  # Show the legend
  plt.legend(prop={"size": 14});

plot_predictions()

"""## 2. Building our first PyTorch model

What our model does:
* Start with random values (weights & Biases)
* Look at the trianing data and adjust the random vlaues to better represent (or get closer to) the ideal values (the weight & bais values we used to create the data)
"""

from torch import nn
import torch
import matplotlib.pyplot as plt

# Create linear regression model class
class LinearRegressionModel(nn.Module): # ← almost everything in PyTorch inherits from
  def __init__(self):
    super().__init__()
    self.weight = nn.Parameter(torch.randn(1,
                                           requires_grad = True,
                                           dytpe = torch.float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad = True,
                                         dtype = torch.float))
    # Forward method to define the computation in the model
    def forward(self, x: torch.tensor) -> torch.Tensor: # ← "x" is the input data
      return self.weight * x + self.bias # this is the linear regression formula mx + c

# 5:05:34 on video